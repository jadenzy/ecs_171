{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a5dd47",
   "metadata": {},
   "source": [
    "Lab1: Complete the TODO parts in the following code. \n",
    "- Using California Housing Dataset from sklearn, select input attributes 1,3,4  as the input features. \n",
    "- Using K-fold cross validation technique (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html), complete the implementation to train a regression model and report performance merics when asked in the following code. \n",
    "- For multiple degrees of model complexity (i.e., degree of polynomial in this exercise) in a for-loop, obtain the model with the minimum reducible_error, polynomial degree, and run the obtained model on the test data. For this part,you should use the split the data into train and test by [75:25] rate and report mse of the final model on test data. \n",
    "- Analyse the results of model performance according to different degrees of polynomial and the number of folds used. You can manipulate the code and share your analysis in terms of the performance of the model (mse and total error), such as for instnace which degree of the model complexity (in relation to the polynomial order) would give a better model? Feel free to include other analysis about the generated models in relation to their performance results. You can event plot the results to support your analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "589fec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: 0.0412, Bias2: 1.3112, Total error: 1.3524\n",
      "Variance: 0.0088, Bias2: 1.1757, Total error: 1.1845\n",
      "Variance: 0.0186, Bias2: 1.6176, Total error: 1.6362\n",
      "Variance: 0.0078, Bias2: 1.2539, Total error: 1.2617\n",
      "Variance: 0.0144, Bias2: 1.5311, Total error: 1.5455\n",
      "The total error of the best model is: 1.1845\n",
      "Degree: 1 MSE: 1.3042670419213096\n",
      "Variance: 0.0593, Bias2: 1.3233, Total error: 1.3827\n",
      "Variance: 0.0267, Bias2: 1.1753, Total error: 1.2020\n",
      "Variance: 0.0287, Bias2: 1.6266, Total error: 1.6553\n",
      "Variance: 0.0319, Bias2: 1.2559, Total error: 1.2878\n",
      "Variance: 0.0229, Bias2: 1.5353, Total error: 1.5582\n",
      "The total error of the best model is: 1.2020\n",
      "Degree: 2 MSE: 1.2992555791229632\n",
      "Variance: 1.3813, Bias2: 1.3039, Total error: 2.6852\n",
      "Variance: 0.0357, Bias2: 1.1765, Total error: 1.2122\n",
      "Variance: 0.0388, Bias2: 1.6263, Total error: 1.6651\n",
      "Variance: 0.0423, Bias2: 1.2547, Total error: 1.2969\n",
      "Variance: 0.0248, Bias2: 1.5294, Total error: 1.5542\n",
      "The total error of the best model is: 1.2122\n",
      "Degree: 3 MSE: 1.2933074099934145\n",
      "Variance: 0.9891, Bias2: 1.3476, Total error: 2.3367\n",
      "Variance: 0.0418, Bias2: 1.1798, Total error: 1.2216\n",
      "Variance: 0.0424, Bias2: 1.6225, Total error: 1.6649\n",
      "Variance: 0.0463, Bias2: 1.2604, Total error: 1.3067\n",
      "Variance: 0.0301, Bias2: 1.5266, Total error: 1.5567\n",
      "The total error of the best model is: 1.2216\n",
      "Degree: 4 MSE: 1.2838771810971725\n",
      "Variance: 0.0662, Bias2: 1.3332, Total error: 1.3994\n",
      "Variance: 0.0502, Bias2: 1.1791, Total error: 1.2293\n",
      "Variance: 0.9144, Bias2: 1.6312, Total error: 2.5456\n",
      "Variance: 22.2122, Bias2: 1.3077, Total error: 23.5199\n",
      "Variance: 0.0383, Bias2: 1.5311, Total error: 1.5694\n",
      "The total error of the best model is: 1.2293\n",
      "Degree: 5 MSE: 1.2947895141859016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "def polynomial_regression(degree, X, y, folds, test_size=0.25, random_state=None):\n",
    "    # Define number of folds for cross-validation\n",
    "    kf = KFold(folds)\n",
    "\n",
    "    # Initialize lists to store results for variance, bias2s, total_error, and models\n",
    "    variance = []\n",
    "    bias2 = []\n",
    "    total_error = []\n",
    "    models = []\n",
    "\n",
    "    # Set the polynomial degree of the model\n",
    "    poly_features = PolynomialFeatures(degree)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, test_index in kf.split(X_poly):\n",
    "        # Split data into training and testing sets for this fold\n",
    "        X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit polynomial regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate variance and Bias^2 for this fold\n",
    "        b2 = np.mean((np.mean(y_pred) - y_test)**2)\n",
    "        var = np.mean((np.mean(y_pred) - y_pred)**2)\n",
    "        total_err = b2 + var\n",
    "        # Append results to lists\n",
    "        variance.append(var)\n",
    "        bias2.append(b2)\n",
    "        total_error.append(total_err)\n",
    "        models.append(model)\n",
    "\n",
    "        # Print results  including variance, Bias^2 and the total_error for this fold. \n",
    "        print(\"Variance: {:.4f}, Bias2: {:.4f}, Total error: {:.4f}\".format(var, b2, total_err))\n",
    "\n",
    "    # print the total_error of the best model\n",
    "    min_error_index = np.argmin(total_error)\n",
    "    best_model = models[min_error_index]\n",
    "    print(\"The total error of the best model is: {:.4f}\".format(total_error[min_error_index]))\n",
    "\n",
    "    \n",
    "    # Testing the final model on the test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=test_size, random_state=42)\n",
    "    # Obtain the predictions on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # store mse score of the model applied on the test data\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "\n",
    "    return mse , best_model\n",
    "\n",
    "\n",
    "# Example usage: load California Housing Dataset and select the first, third, and forth attributes as input features in X\n",
    "housing = fetch_california_housing()\n",
    "target = housing.target\n",
    "X = housing.data[:, [1, 3, 4]]\n",
    "# Set the target valiable \n",
    "\n",
    "# degrees = range(1, 10) # for testing \n",
    "degrees = range(1, 6)  # Try polynomial degrees from 1 to 5\n",
    "# Try degrees from 1 to 5 and in a loop, report mse of the best model trained using k-fold cross validation and print(\"Degree:\", degree, \"MSE:\", mse)\n",
    "for degree in degrees:\n",
    "    mse, best_model = polynomial_regression(degree, X, target, 5)\n",
    "    print(\"Degree:\", degree, \"MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82509c15",
   "metadata": {},
   "source": [
    "- In the above test, the 4th degree produces the lowest MES which indicates it is the closest to the predict value. However, the best model occures in the 1st degree. \n",
    "- The total error increases as the degree increases. \n",
    "- In the above code, we know with 1st degree : best model, 4th degee: stable model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ae884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more detail testing on more degree and 3 different folding \n",
    "\n",
    "def polynomial_regression_test(degree, X, y, folds, test_size=0.25, random_state=None):\n",
    "    # Define number of folds for cross-validation\n",
    "    kf = KFold(folds)\n",
    "\n",
    "    # Initialize lists to store results for variance, bias2s, total_error, and models\n",
    "    variance = []\n",
    "    bias2 = []\n",
    "    total_error = []\n",
    "    models = []\n",
    "\n",
    "    # Set the polynomial degree of the model\n",
    "    poly_features = PolynomialFeatures(degree)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, test_index in kf.split(X_poly):\n",
    "        # Split data into training and testing sets for this fold\n",
    "        X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit polynomial regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate variance and Bias^2 for this fold\n",
    "        b2 = np.mean((np.mean(y_pred) - y_test)**2)\n",
    "        var = np.mean((np.mean(y_pred) - y_pred)**2)\n",
    "        total_err = b2 + var\n",
    "        # Append results to lists\n",
    "        variance.append(var)\n",
    "        bias2.append(b2)\n",
    "        total_error.append(total_err)\n",
    "        models.append(model)\n",
    "\n",
    "    # print the total_error of the best model\n",
    "    min_error_index = np.argmin(total_error)\n",
    "    best_model = models[min_error_index]\n",
    "    print(\"The total error of the best model is: {:.4f}\".format(total_error[min_error_index]))\n",
    "\n",
    "    \n",
    "    # Testing the final model on the test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=test_size, random_state=42)\n",
    "    # Obtain the predictions on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # store mse score of the model applied on the test data\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return mse , best_model\n",
    "\n",
    "\n",
    "# Example usage: load California Housing Dataset and select the first, third, and forth attributes as input features in X\n",
    "housing = fetch_california_housing()\n",
    "target = housing.target\n",
    "X = housing.data[:, [1, 3, 4]]\n",
    "# Set the target valiable \n",
    "\n",
    "degrees = range(1, 10) # for testing \n",
    "folds = [5, 10, X.shape[0]] \n",
    "# Try degrees from 1 to 5 and in a loop, report mse of the best model trained using k-fold cross validation and print(\"Degree:\", degree, \"MSE:\", mse)\n",
    "for degree in degrees:\n",
    "    print(\"Degree: \", degree)\n",
    "    for fold in folds: \n",
    "        mse, best_model = polynomial_regression_test(degree, X, target, fold)\n",
    "        print(\"Fold:\", fold, \"; \", \"MSE:\", mse)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
